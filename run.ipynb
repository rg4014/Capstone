{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa3faa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import entropy, spearmanr, wasserstein_distance\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Load merged results\n",
    "df = pd.read_csv(\"merged_scores.csv\")\n",
    "# Example: each row = one candidate–JD pair or one candidate–main JD pair\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3933dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_shift(df, col_ref, col_comp):\n",
    "    return (df[col_comp] - df[col_ref]).mean()\n",
    "\n",
    "def var_ratio(df, col_ref, col_comp):\n",
    "    var_ref = df[col_ref].var()\n",
    "    var_comp = df[col_comp].var()\n",
    "    return var_comp / var_ref if var_ref != 0 else np.nan\n",
    "\n",
    "print(\"Unfair vs Baseline – mean shift:\",\n",
    "      mean_shift(df, \"baseline_score\", \"unfair_score\"))\n",
    "print(\"ITI vs Baseline – mean shift:\",\n",
    "      mean_shift(df, \"baseline_score\", \"iti_fair_score\"))\n",
    "\n",
    "print(\"Unfair vs Baseline – variance ratio:\",\n",
    "      var_ratio(df, \"baseline_score\", \"unfair_score\"))\n",
    "print(\"ITI vs Baseline – variance ratio:\",\n",
    "      var_ratio(df, \"baseline_score\", \"iti_fair_score\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf110ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_gap(df, group_col, score_col):\n",
    "    group_means = df.groupby(group_col)[score_col].mean().dropna()\n",
    "    if len(group_means) < 2:\n",
    "        return np.nan\n",
    "    # For binary group (e.g. Male/Female)\n",
    "    # take max minus min\n",
    "    return group_means.max() - group_means.min()\n",
    "\n",
    "for col in [\"baseline_score\", \"unfair_score\", \"iti_fair_score\"]:\n",
    "    print(f\"BiasGap gender, {col}:\",\n",
    "          bias_gap(df, \"gender\", col))\n",
    "    print(f\"BiasGap race, {col}:\",\n",
    "          bias_gap(df, \"race\", col))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9ce0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_hist_probs(series, bins=20, eps=1e-9):\n",
    "    counts, bin_edges = np.histogram(series.dropna(), bins=bins, range=(0, 100), density=False)\n",
    "    probs = counts.astype(float)\n",
    "    probs = probs / probs.sum() if probs.sum() > 0 else probs\n",
    "    # avoid 0 for KL\n",
    "    probs = np.clip(probs, eps, 1.0)\n",
    "    return probs, bin_edges\n",
    "\n",
    "def kl_divergence(ref, comp):\n",
    "    p, _ = score_hist_probs(ref)\n",
    "    q, _ = score_hist_probs(comp)\n",
    "    return entropy(p, q)\n",
    "\n",
    "def emd(ref, comp):\n",
    "    # Wasserstein distance over score values\n",
    "    return wasserstein_distance(ref.dropna(), comp.dropna())\n",
    "\n",
    "def spearman_ref(ref, comp):\n",
    "    return spearmanr(ref, comp, nan_policy=\"omit\").correlation\n",
    "\n",
    "print(\"KL(unfair || baseline):\", kl_divergence(df[\"baseline_score\"], df[\"unfair_score\"]))\n",
    "print(\"KL(iti || baseline):\", kl_divergence(df[\"baseline_score\"], df[\"iti_fair_score\"]))\n",
    "\n",
    "print(\"EMD(unfair, baseline):\", emd(df[\"baseline_score\"], df[\"unfair_score\"]))\n",
    "print(\"EMD(iti, baseline):\", emd(df[\"baseline_score\"], df[\"iti_fair_score\"]))\n",
    "\n",
    "print(\"Spearman(baseline, unfair):\", spearman_ref(df[\"baseline_score\"], df[\"unfair_score\"]))\n",
    "print(\"Spearman(baseline, iti):\", spearman_ref(df[\"baseline_score\"], df[\"iti_fair_score\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fd9a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(df[\"baseline_score\"].dropna(), bins=20, alpha=0.5, label=\"Baseline\")\n",
    "plt.hist(df[\"unfair_score\"].dropna(), bins=20, alpha=0.5, label=\"Unfair\")\n",
    "plt.hist(df[\"iti_fair_score\"].dropna(), bins=20, alpha=0.5, label=\"ITI Fair\")\n",
    "plt.xlabel(\"Score\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Score Distributions – Baseline vs Unfair vs ITI\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c69403",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for gender, sub in df.groupby(\"gender\"):\n",
    "    plt.hist(sub[\"unfair_score\"].dropna(), bins=20, alpha=0.4, label=f\"Unfair – {gender}\")\n",
    "plt.xlabel(\"Unfair Score\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Unfair Model Scores by Gender\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "for gender, sub in df.groupby(\"gender\"):\n",
    "    plt.hist(sub[\"iti_fair_score\"].dropna(), bins=20, alpha=0.4, label=f\"ITI – {gender}\")\n",
    "plt.xlabel(\"ITI Fair Score\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"ITI Model Scores by Gender\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf26cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top1_accuracy(df_multi, score_col):\n",
    "    # argmax JD per candidate\n",
    "    idx = df_multi.groupby(\"candidate_id\")[score_col].idxmax()\n",
    "    pred = df_multi.loc[idx, [\"candidate_id\", \"jd_industry\", \"true_industry\"]]\n",
    "    return (pred[\"jd_industry\"] == pred[\"true_industry\"]).mean()\n",
    "\n",
    "for col in [\"baseline_score\", \"unfair_score\", \"iti_fair_score\"]:\n",
    "    acc = top1_accuracy(df_multi, col)\n",
    "    print(f\"Top-1 accuracy ({col}):\", acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2f5ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def industry_auc(df_multi, score_col):\n",
    "    aucs = []\n",
    "    for cid, sub in df_multi.groupby(\"candidate_id\"):\n",
    "        y_true = (sub[\"jd_industry\"] == sub[\"true_industry\"]).astype(int)\n",
    "        if y_true.nunique() < 2:\n",
    "            continue\n",
    "        try:\n",
    "            auc = roc_auc_score(y_true, sub[score_col])\n",
    "            aucs.append(auc)\n",
    "        except ValueError:\n",
    "            continue\n",
    "    return np.mean(aucs) if aucs else np.nan\n",
    "\n",
    "for col in [\"baseline_score\", \"unfair_score\", \"iti_fair_score\"]:\n",
    "    auc = industry_auc(df_multi, col)\n",
    "    print(f\"Industry AUC ({col}):\", auc)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
